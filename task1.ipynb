{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task1、随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.集成学习的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成学习：\n",
    "    使用一系列学习器进行学习，并使用某种规则把各个学习结果进行整合，从而获得比单一学习器泛化性能更好的学习器。集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.个体学习器的概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 个体学习器：\n",
    "     个体学习器是一个相对概念，在集成学习中，集成之前的学习器称为个体学习器。通常由现有算法（如决策树算法、BP神经网络算法等）产生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.boosting bagging的概念、异同点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### boosting :\n",
    "    Boosting，它的工作机制类似：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器（相当于挑分错的出来训练）；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。\n",
    "### bagging: \n",
    "    Bagging是并行式集成学习方法最著名的代表。把样本做K次有放回的抽样；每次抽样训练一个模型共K个；用K个模型投票或均值作为最后结果；\n",
    "### 异同点：\n",
    "    相同点 -- 都是将已有的效果偏差的模型通过一定方式进行组合形成一个更强的分类器。 \n",
    "    不同点 -- 1.取样方式不同，boosting是根据错误率取样，bagging是随机放回取样。从这个基点讲，boosting的精度会优于bagging,但有一些数据集中，boosting会退化-overfit。boosting思想的一种改进型adaboost方法在邮件过滤，文本分类中有很好的性能。\n",
    "    2.bagging的各个预测函数没有权重，而boosting有权重； baging分类器的权重是一致的，boosting的分类器权重不一样(准确率高的分类器权重越大) bagging的各个函数可以并行生成，而boosting的各个预测函数只能按顺序生成。对于像神经网络这样极为消耗时间的算法，bagging可通过并行节省大量的时间开销。bagging和boosting都可以有效地提高分类的准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.理解不同的结合策略(平均法，投票法，学习法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 平均法\n",
    "    主要针对回归模型，将模型结果取平均值最为最后的预测结果。\n",
    "    分为简单平均法和加权平均法\n",
    "    简单平均是加权平均的特例，权重一般是从训练数据中学习而得，显示任务中的训练样本通常不充分或者存在噪声，这将使得学出的权重不完全可靠，所以加权平均未必一定由于简单平均法。一般而言，在个体学习器性能差异较大时宜使用加权平均法，而在个体学习器性能相近时宜使用简单平均法。\n",
    "### 投票法\n",
    "    主要针对分类模型，分类结果中出现次数最多的类型作为最终分类结果。\n",
    "    绝对多数投票法：某标记超过半数。\n",
    "    相对多数投票法：预测为得票最多的标记，若同时有多个标记获得最高票，则从中随机选取一个。\n",
    "    加权投票法：提供了预测结果，与加权平均法类似。\n",
    "### 学习法（Stacking）\n",
    "    不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器。也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.随机森林的思想"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 概念\n",
    "    随机森林在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，传统决策树在选择划分属性时是在当前节点的属性集合（假设有d个属性）中选择一个最优属性；而在随机森林中，对及决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。\n",
    "    用bagging的思想选出K个训练子集，对每个子集用决策树作为分类器进行训练，但是决策树入模的变量也是从全部变量中随机取一部分出来的。最终训练出K个决策树组成森林。个人理解随机主要体现在训练集选取随机和训练变量随机。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.随机森林的推广"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 极限树（Extremely Randomized Trees）\n",
    "    再加上一个随机化步骤，就会得到极限随机树，每棵树都使用整个学习样本进行了训练，其次，自上而下的划分是随机的。它并不计算每个特征的最优划分点（例如，基于信息熵或者基尼不纯度），而是随机选择划分点。该值是从特征经验范围内均匀随机选取的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.随机森林的优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点\n",
    "    对于很多种数据，它可以产生高准确度的分类器。\n",
    "    它可以处理大量的输入变量。\n",
    "    它可以在决定类别时，评估变量的重要性。\n",
    "    在建造森林时，它可以在内部对于一般化后的误差产生不偏差的估计。\n",
    "    它包含一个好方法可以估计丢失的数据，并且，如果有很大一部分的数据丢失，仍可以维持准确度。\n",
    "    它提供一个实验方法，可以去侦测variable interactions。\n",
    "    对于不平衡的分类数据集来说，它可以平衡误差。\n",
    "    它计算各例中的亲近度，对于数据挖掘、侦测离群点（outlier）和将数据可视化非常有用。\n",
    "    使用上说。它可被延伸应用在未标记的数据上，这类数据通常是使用非监督式聚类。也可侦测偏离者和观看数据。\n",
    "    学习过程是很快速的。\n",
    "### 缺点\n",
    "    随机森林更偏向于那些取值个数较多的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.随机森林在sklearn中的参数解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators='warn',criterion='gini', max_depth=None, min_samples_split=2,\n",
    "min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False,\n",
    "class_weight=None)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators：integer, optional (default=10，0.22版本默认值为100)；随机森林中用到树的数量 criterion : string,optional(default=\"mse\")；测量分割的好坏，mae(均方误差)/mae(平均绝对误差) max_depth : integer or None,optional (default=None)；树的最大深度 min_samples_split : int,float,optional(default=2)；分割一个内部节点所需最小样本数 min_samples_leaf : int,float,optional(default=1)；叶节点上所需最小样本数 min_weight_fraction_leaf : float,optional(default=0.)叶节点所需的(所有输入样本的)权值之和的最小加权分数，当其为0(默认情况)时所有样本具有相同的权重 max_features : int,float,string(\"auto\",\"sqrt\",\"log2\") or None,optional(default=\"auto\")；每次在寻找最佳分割时需要考虑的特性数量 max_leaf_nodes : int or None, optional (default=None)；最大叶节点数 min_impurity_decrease : float, optional (default=0.)；最小的杂质减少量，若一个节点能通过分割把杂质的量降到这个数值以下，那么分割就会进行 min_impurity_split : float, (default=1e-7)；树早期停止生长的阈值，若一个节点的杂质超过阈值则会进行分裂，否则就是叶节点 bootstrap : boolean, optional (default=True)；构建树时是否使用bootstrap samples oob_score : bool, optional (default=False)；是否使用袋外样本来估计不可示数据的R^2 n_jobs : int or None, optional (default=None)；fit和predict并行运算的作业数 random_state : int, RandomState instance or None, optional (default=None)；随机种子 verbose : int, optional (default=0)；在拟合和预测时控制详细程度 warm_start : bool, optional (default=False)；为True时重复使用上一个fit后的model并向整体添加更多的估算器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.随机森林的应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 缺失值填充\n",
    "2) 特征选择\n",
    "3) 模型预测、分类"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
