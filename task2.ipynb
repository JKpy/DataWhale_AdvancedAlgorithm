{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task2、GBDT算法梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.前向分布算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向分布算法基本思想\n",
    "    从前往后，一步只学习一个基函数，逐步逼近优化目标函数。\n",
    "    每步只需要优化当前的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.负梯度拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    顾名思义，就是在损失函数梯度的负方向上进行学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    GBDT 主要依据不同的损失函的梯度进行更新，针对回归与分类问题，有不同的损失函数如下：\n",
    "    回归问题\n",
    "        平方损失函数\n",
    "             L(y,f(x))=(y−f(x))^2\n",
    "        绝对损失函数\n",
    "             L(y,f(x))=∣y−f(x)∣\n",
    "    分类问题\n",
    "         指数函数损失函数\n",
    "             L(y,f(x))=e^(−y*f(x))\n",
    "    对数损失函数(对数似然损失函数)\n",
    "         二分类问题\n",
    "             L(y,f(x))=log(1+exp(−y*f(x)))\n",
    "        多分类问题\n",
    "             L(y,f(x))=−∑_(K,k=1)[y_k*log(p_k(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    回归树总体流程类似于分类树，区别在于，回归树的每一个节点都会得到一个预测值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.二分类，多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     这里我们再看看GBDT分类算法，GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。\n",
    "     为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数（上述损失函数所介绍）的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。除了负梯度计算，其他过程和回归梯度拟合一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT正则化主要有三种方法。\n",
    "    1）对样本设置采样比例，可以减小方差，但是采样率太小会增大偏差\n",
    "    2）添加learning rate\n",
    "    3）对子树进行正则化剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优点：\n",
    "    精度高\n",
    "    能处理非线性数据\n",
    "    能处理多特征类型\n",
    "    适合低维稠密数据\n",
    "### 缺点：\n",
    "    不好并行处理（因为上下两颗树有联系）\n",
    "    多分类的时候 复杂度很大\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.sklearn参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "GBDC = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     loss：损失函数  {'deviance', 'exponential'}, optional (default='deviance')\n",
    "     learning_rate：学习率 float, optional (default=0.1)\n",
    "     n_estimators：基础分类器数量 int (default=100)\n",
    "     subsample：训练基础分类器样本比例 float, optional (default=1.0)\n",
    "     criterion：划分衡量指标 string, optional (default=\"friedman_mse\")\n",
    "     min_samples_split：决策树叶结点继续分裂最小样本数量 int, float, optional (default=2)\n",
    "     min_samples_leaf：决策树叶结点最小样本数量  int, float, optional (default=1)\n",
    "     min_weight_fraction_leaf：决策树叶结点最小加权样本数量 float, optional (default=0.)\n",
    "     max_depth：决策树最大深度 integer, optional (default=3)\n",
    "     min_impurity_decrease：决策树叶结点最小衡量指标提升 float, optional (default=0.)\n",
    "     min_impurity_split : 节点划分最小不纯度 float, (default=1e-7)\n",
    "     init：初始化方法 estimator, optional\n",
    "     random_state：随机种子 int, RandomState instance or None, optional (default=None)\n",
    "     max_features：搜索划分时考虑的特征数量 int, float, string or None, optional (default=None)\n",
    "     verbose：控制输出   int, default: 0\n",
    "     max_leaf_nodes：决策树最大叶结点数量 int or None, optional (default=None)\n",
    "     warm_start：是否使用之前的输出   bool, default: False\n",
    "     presort：是否尝试预先排序数据  bool or 'auto', optional (default='auto')\n",
    "     validation_fraction：验证集比例，控制算法是否提前结束  float, optional, default 0.1\n",
    "     n_iter_no_change：控制算法是否提前结束 int, default None\n",
    "     tol：控制算法是否提前结束 float, optional, default 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    常见分类问题、回归问题均可使用GBDT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
